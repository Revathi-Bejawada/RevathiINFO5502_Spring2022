{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Revathi-Bejawada/RevathiINFO5502_Spring2022/blob/main/lab_assignment_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQzJvk2PxcHV"
      },
      "source": [
        "## The third Lab-assignment (02/10/2022, 50 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uNeDjeBxcHZ"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hU-hsYWxcHa"
      },
      "source": [
        "Question 1 (10 points). Fomulate your domain problem: Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZiiz5dmxcHa"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "Domain Problem:\n",
        "\n",
        "My domain problem is to do TV ratings scraping and analysis of ratings to find the best TV based on the customer reviews\n",
        "\n",
        "In this we are webscrapping the data from ecommerce website Flipkart we will be considering only the reviews I have seen the inspect html code and I have considered only the reviews.\n",
        "\n",
        "For our analysis we have considered 1000 samples of data\n",
        "\n",
        "Steps for collecting and saving the data:\n",
        "\n",
        "1.We use web scraping technique to scrape the data.\n",
        "2.We use requests library to send the request to the website\n",
        "3.We use BeautifulSoup library.\n",
        "4.There are 1,591 results for Tv Applications.\n",
        "5.We parse through each page and go to the reviews.\n",
        "6.We get the reviews from the product pages.\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVYRrqpJxcHc"
      },
      "source": [
        "Question 2 (10 points). Collect your data to answer the research problem: Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5pr3YGXxcHc",
        "outputId": "09de7bc8-001d-4922-ae6c-c6bc7441c963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***__Page Number = 1__***\n",
            "39\n",
            "***__Page Number = 2__***\n",
            "78\n",
            "***__Page Number = 3__***\n",
            "117\n",
            "***__Page Number = 4__***\n",
            "156\n",
            "***__Page Number = 5__***\n",
            "195\n",
            "***__Page Number = 6__***\n",
            "234\n",
            "***__Page Number = 7__***\n",
            "272\n",
            "***__Page Number = 8__***\n",
            "311\n",
            "***__Page Number = 9__***\n",
            "349\n",
            "***__Page Number = 10__***\n",
            "386\n",
            "***__Page Number = 11__***\n",
            "421\n",
            "***__Page Number = 12__***\n",
            "458\n",
            "***__Page Number = 13__***\n",
            "494\n",
            "***__Page Number = 14__***\n",
            "494\n",
            "***__Page Number = 15__***\n",
            "527\n",
            "***__Page Number = 16__***\n",
            "562\n",
            "***__Page Number = 17__***\n",
            "595\n",
            "***__Page Number = 18__***\n",
            "629\n",
            "***__Page Number = 19__***\n",
            "660\n",
            "***__Page Number = 20__***\n",
            "695\n",
            "***__Page Number = 21__***\n",
            "730\n",
            "***__Page Number = 22__***\n",
            "764\n",
            "***__Page Number = 23__***\n",
            "796\n",
            "***__Page Number = 24__***\n",
            "830\n",
            "***__Page Number = 25__***\n",
            "866\n",
            "***__Page Number = 26__***\n",
            "901\n",
            "***__Page Number = 27__***\n",
            "931\n",
            "***__Page Number = 28__***\n",
            "963\n",
            "***__Page Number = 29__***\n",
            "995\n",
            "***__Page Number = 30__***\n",
            "1000\n",
            "\n",
            "product ratings: ['4.5', '4.5', '4.3', '4.4', '4.2', '4.4', '4.3', '4.3', '4.2', '4.3', '4.4', '4.4', '4.5', '4.4', '4.3', '4.4', '4.2', '4.3', '4.4', '4.3', '4.3', '4.4', '4.2', '4.7', '4.5', '5', '1', '4.5', '5', '5', '4.3', '5', '5', '4.2', '4', '5', '4.4', '4', '5', '4.4', '4.5', '4.4', '4.4', '4.5', '4.3', '4.3', '4.5', '4.2', '4.4', '4.2', '4.5', '4.4', '4.3', '4.4', '4.3', '4.5', '4.2', '4.4', '4.4', '4.4', '4.4', '4.4', '4.4', '4.4', '4', '5', '4.5', '5', '5', '4.4', '5', '5', '4.4', '4', '3', '4.3', '5', '5', '4.2', '4.4', '4.5', '4.4', '4.3', '4.1', '4.4', '4.5', '4.1', '3.8', '4.6', '4.5', '4.2', '4.6', '4.4', '4.3', '4.3', '4.6', '4.3', '4.4', '4.4', '4.4', '4.3', '4.7', '4.1', '5', '4', '3.8', '5', '4', '4.4', '4', '5', '4.1', '4', '1', '4.5', '5', '5', '4.4', '4.4', '4.1', '4.4', '4.4', '4.3', '4.4', '4.5', '4.4', '4.7', '4.2', '4.5', '4.3', '4.4', '4.4', '4.3', '4.4', '4.1', '4.4', '4.4', '4.5', '4.4', '4.4', '4.3', '4.5', '5', '5', '4.2', '3', '1', '4.1', '5', '1', '4.5', '5', '5', '4.4', '5', '5', '4.4', '4.3', '4.4', '4.4', '4.4', '4.3', '3.8', '4.5', '4.3', '4.3', '3.8', '4.5', '4.4', '4.3', '4.1', '4.3', '4.8', '4.6', '4.3', '4.4', '4.4', '4.2', '4.3', '4.4', '4.5', '5', '5', '4.3', '4', '5', '4.3', '4', '5', '4.2', '5', '4', '4.5', '5', '5', '4.3', '4.6', '4.4', '4.4', '4.4', '4.4', '4.4', '4.5', '4.5', '4.4', '4.4', '4.5', '4.3', '4', '4.4', '4.3', '4.3', '3.7', '4.4', '4.4', '4.4', '4.4', '4.7', '4.3', '4.6', '5', '5', '4.4', '5', '5', '4.3', '5', '5', '4.5', '5', '5', '4.4', '3', '5', '4.3', '4.1', '4.2', '4.4', '4.4', '4.4', '4.3', '4.5', '4.4', '4.4', '4.5', '4.5', '4.3', '4.6', '4.3', '4.3', '4.4', '4.4', '4.4', '4.4', '4.2', '4.3', '4.4', '4.5', '5', '4', '4.3', '4', '2', '4.3', '5', '5', '4.5', '5', '5', '4.4', '4', '3', '4.5', '4.6', '4.5', '4.4', '4.2', '4.3', '4.4', '4.5', '4.4', '4.3', '4.4', '4.5', '4.3', '4.3', '4.4', '4.3', '4.2', '4.4', '4.2', '4.4', '3.8', '4.3', '4.4', '4.4', '4.3', '1', '5', '4.5', '5', '5', '4.3', '5', '5', '4.2', '4', '5', '4.5', '5', '5', '4.6', '4.4', '4.5', '4.4', '4.3', '4.6', '3.9', '4.5', '4.4', '4.3', '4.5', '4.2', '4.3', '4.4', '4.3', '4.3', '4.5', '4.4', '4.4', '4.3', '4.1', '4.3', '4.4', '4.6', '5', '4', '4.3', '5', '3', '4.4', '4', '5', '4.5', '5', '5', '4.3', '1', '4', '4.4', '4.5', '4.8', '4.4', '3.9', '4.2', '4.3', '4.5', '4.3', '4.3', '4.2', '4.5', '4.4', '4.8', '4.4', '4.3', '4.4', '4.4', '4.4', '4.4', '3.5', '4.4', '4.4', '4', '3', '4.5', '5', '5', '4.3', '5', '5', '4.3', '5', '5', '4.4', '4', '4', '4.5', '4.5', '4.2', '4.4', '4.8', '4.5', '4.4', '4.4', '4.5', '5', '4.6', '4.3', '3.5', '4.2', '4.4', '4.4', '4.4', '4.2', '4.8', '4.4', '4.5', '4', '3', '4.8', '5', '5', '4.5', '5', '5', '4.4', '4', '4', '4.6', '5', '5', '4.2', '5', '4.4', '4.4', '4.2', '4.3', '4.2', '4.5', '4.2', '4.4', '4.3', '4.5', '4.3', '4.3', '4.3', '4.3', '5', '4.4', '4.4', '4.8', '4.6', '4.3', '4.4', '5', '1', '4.4', '4', '5', '4.5', '5', '5', '4.8', '5', '5', '4.3', '4', '4', '4.4', '4.5', '5', '4.4', '3.6', '4.3', '4.5', '4.3', '4.4', '4.2', '4.5', '4.5', '4.9', '4.7', '4.3', '4.6', '4.4', '4.1', '4.4', '4.8', '4.4', '4.5', '5', '5', '4.4', '4', '5', '4.5', '5', '4', '4.5', '5', '5', '4.4', '5', '4', '4.6', '5', '4.2', '4.4', '4.5', '4.1', '4.5', '4.1', '4.5', '3', '2.3', '4.3', '4.4', '4', '4.4', '5', '4.2', '4.4', '4.5', '5', '5', '4.4', '5', '4', '4.4', '5', '5', '4.3', '5', '5', '4', '4', '4', '4.4', '4.2', '4.1', '4.4', '5', '4.9', '4.5', '4.7', '4.4', '4.5', '3.9', '4.8', '4', '4.3', '4.7', '4.8', '4.4', '5', '2.3', '4.4', '4.5', '5', '5', '4.1', '5', '4', '4.4', '5', '5', '4.7', '4', '4', '4.4', '5', '5', '4.5', '4.7', '4.4', '4.2', '4.4', '4.7', '4.5', '4.7', '4.4', '4.5', '3.6', '4.1', '4.3', '4.1', '4.4', '5', '4', '4.4', '4.5', '5', '5', '4.4', '5', '5', '4.7', '3', '3', '4.3', '5', '5', '4', '5', '4', '4.5', '4.4', '4', '4.4', '4.6', '4.3', '4.5', '4.3', '4.5', '5', '4.5', '4.4', '4.4', '4.3', '4.4', '4.4', '3.7', '4.1', '4.3', '4.3', '5', '3', '4.5', '5', '5', '4.4', '5', '5', '4.4', '5', '3', '4.4', '5', '1', '4.5', '3.9', '4.4', '4', '4.5', '4.5', '4.2', '4.3', '4.2', '4.5', '4.3', '4.4', '4.3', '4.4', '4.7', '4.4', '4.5', '5', '5', '4.4', '3', '5', '4.4', '5', '5', '4.3', '5', '5', '4.3', '5', '5', '4.7', '4.4', '4.3', '4.6', '4.5', '4.5', '4.3', '4.4', '4.5', '4.3', '4.4', '4.6', '4.3', '4.5', '4.1', '5', '4.4', '4.4', '4.7', '4.4', '4.5', '5', '5', '4.3', '4', '5', '4.4', '5', '5', '4.3', '5', '5', '4.5', '2', '5', '2.5', '4.5', '4.4', '4.2', '4.3', '4', '4.5', '3.9', '4.2', '4', '4.3', '4.7', '3.6', '3', '4.4', '3.7', '4.4', '4.4', '4.5', '4.3', '4.2', '5', '4', '4.5', '5', '5', '3', '5', '5', '3.7', '5', '5', '4.4', '5', '5', '4.6', '4.5', '4.3', '4.4', '4.8', '4.5', '4.2', '4.4', '4.5', '4.3', '4.7', '4.3', '4.5', '4.5', '4.4', '4.4', '4.6', '4.4', '4.4', '4.5', '5', '5', '4.4', '5', '5', '4.3', '5', '5', '4.5', '5', '2', '4.3', '5', '1', '4.4', '4.2', '4.3', '4.4', '4.7', '4.3', '4.5', '4.4', '4.5', '4.5', '4.3', '4.3', '4.3', '4.4', '3.9', '3', '4.4', '4.3', '4', '4', '4.5', '5', '5', '4.4', '5', '5', '4.3', '5', '4', '4.4', '5', '5', '4.2', '4.4', '4.4', '4', '4.3', '4.5', '4', '4.8', '4.5', '4.2', '4.6', '4.3', '3.8', '4.4', '4.4', '4.3', '4.3', '4.8', '4.4', '4.5', '5', '5', '4.2', '5', '5', '4.3', '5', '1', '4.2', '5', '1', '4', '5', '5', '4.3', '4.5', '4.4', '4.3', '4.4', '4.5', '3.4', '4.1', '4.8', '4.5', '5', '4.3', '4.5', '4.3', '4.4', '4.8', '4.4', '4', '4.2', '4.3', '4.4', '4.5', '4', '4', '3.4', '2', '5', '4.5', '5', '5', '4.3', '5', '1', '4.3', '5', '5', '4.2', '3.9', '4', '4.4', '4.4', '4.1', '4.5', '4.4', '4.7', '4.5', '4.5', '3.8', '3.7', '4.3', '4.2', '4.4', '3.8', '4.4', '3.7', '4.3', '4', '1', '1', '4.5', '5', '5', '3.8', '5', '4', '4.3', '5', '5', '4.1', '5', '5', '4.3', '4.4', '4.1', '4.5', '4.5', '4', '4.4', '4.6', '4.3', '4.5', '4.4', '4.2', '4.6', '4', '4.4', '4.5', '5', '5', '4.4', '5', '5', '4.3', '5', '5', '4.4', '4', '5', '4.6', '5', '1', '4.4', '4.1', '4.4', '4.1', '4.5', '4.2', '4', '3.9', '4.5', '4.4', '4.3', '4.1', '4.3', '4.4', '4', '4.1', '4.4', '4.5', '5', '5', '4.1', '5', '5', '4.4', '4', '5', '4.1', '3', '1', '4.4', '5', '5', '4.4', '4.5', '4.2', '3.8', '4.4', '4.5', '2.9', '4.3', '4.3', '4.5', '4', '3.9', '4.4', '4.1', '3.1', '3.8', '4.4', '4.2', '5', '5', '4.5', '5', '5', '4', '4', '2', '3.1', '5', '5', '4.5', '5', '1', '4.4', '4.4', '4.4', '4.1', '4.5']\n",
            "\n",
            "1000 data samples are collected\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "from bs4 import BeautifulSoup \n",
        "\n",
        "import urllib\n",
        "from urllib.request import Request, urlopen\n",
        "\n",
        "from urllib.error import HTTPError\n",
        "import json\n",
        "import re\n",
        "\n",
        "#total_count = 0\n",
        "numb = 0\n",
        "\n",
        "required_ratings = []\n",
        "website_ = \"https://www.flipkart.com/search?q=TV&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page={}\"\n",
        "for p in range(1, 100):\n",
        "    # we create the data soup of the main url page html elements.\n",
        "\n",
        "    req_link = Request(website_.format(p), headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    s = urllib.request.urlopen(req_link) #requesting\n",
        "\n",
        "    input = s.read()\n",
        "    r = BeautifulSoup(input)\n",
        "\n",
        "    print(\"***__Page Number = {}__***\".format(p))\n",
        "\n",
        "    # we will iterate through pages of the  search results till we get 1000 records\n",
        "    for request_link in r.find_all('div', attrs = {'class': \"_3LWZlK\"}): #getting our ratings div from website inspect html code\n",
        "\n",
        "        required_ratings.append(request_link.text)\n",
        "\n",
        "        numb = numb + 1\n",
        "        if numb == 1000:\n",
        "            break\n",
        "            \n",
        "    print(numb)\n",
        "    if numb == 1000:\n",
        "        break\n",
        "\n",
        "print()\n",
        "print('product ratings:', required_ratings) #here at last we are priting all the ratings \n",
        "print()\n",
        "print('1000 data samples are collected')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsG4nmEAxcHd"
      },
      "source": [
        "Question 3 (10 points). Understand the data quality: Search a second hand dataset (any dataset) from kaggle or other websites. Describe the data quality problem of the dataset and explain your strtegy to clean the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVMRSqK0xcHd"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "'''\n",
        "\n",
        "Please write you answer here:\n",
        "\n",
        "Everyone are aware of the pandemic that we are facing right now, COVID-19. \n",
        "\n",
        "I have taken a covid dataset from kaggle.\n",
        "\n",
        "US counties COVID 19 dataset: https://www.kaggle.com/fireballbyedimyrnmom/us-counties-covid-19-dataset\n",
        "\n",
        "The dataset has 6 columns\n",
        "\n",
        "The first thing we need to do right after we take a dataset is understand the business problem\n",
        "\n",
        "This dataset has some missing(null) values and some character encodings\n",
        "\n",
        "My Strategy to clean the data:\n",
        "\n",
        "1. See how many missing data points we have\n",
        "2. Figure out why the data is missing\n",
        "3. Drop missing values\n",
        "4. Filling in missing values \n",
        "5. Removing the character encoding values\n",
        "6. Convert our date columns to datetime\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8hYLKBpxcHe"
      },
      "source": [
        "Question 4 (20 points). Data cleaning: There are two datasets TwADR-L (from Twitter) and AskAPatient (Link: https://zenodo.org/record/55013#.YgU2NN-ZO4T) for medical concept\n",
        "normalization. However, the two datasets have serious data quality problems. Please read the introduction of the datasets and clean the two datasets by following the steps in the statement.\n",
        "\n",
        "In the original dataset, the TwADR-L had 48,057 training, 1,256 validation and 1,427 test examples. The test set (all\n",
        "test samples from 10 folds combined) consists of 765 unique phrases and 273 unique classes (medical concepts). The AskAPatient dataset contained 156,652 training, 7,926 validation, and 8,662 test examples. The entire test set (all test samples\n",
        "from 10 folds combined) consists of 3,749 unique phrases and 1,035 unique classes (medical concepts). The authors\n",
        "randomly split each dataset into ten equal folds, ran 10-fold cross validation and reported the accuracy averaged across the\n",
        "ten folds. \n",
        "\n",
        "We found that, in the original data set, many phrase-label pairs appeared multiple times within the same training data file\n",
        "and also across the training and test data sets in the same fold. In the AskAPatient data set, on average 35.82% of the test data overlapped with training data in the same fold. In the Twitter (TwADR-L) dataset, on average 8.62% of the test set had an overlap with the training data in the same fold. Having a large overlap between the training and the test data can potentially\n",
        "introduce bias in the model and contribute to high accuracy. It is not unlikely that the high model performance reported in the original paper may be triggered by the the large overlap between the training and test sets.\n",
        "\n",
        "Therefore to remove the bias, we further cleaned and recreated the training, validation, and test sets such that each\n",
        "phrase-label pair appears only once in the entire dataset (either in training, validation or test set).\n",
        "\n",
        "(1) First, we combined all examples in training, validation and test data from the original data set and then removed all\n",
        "duplicate phrase-label pairs (examples that have the same phrase and label pair and appear more than once in training/validation/test datasets). Table II shows the statistics of the new dataset (after removing duplicates). The Twitter data set had 3,157 unique phrase-label pairs and 2,220 unique labels (medical concepts) while 173 phrases had multiple labels (i.e., they were assigned to more than one label). Many concepts had only one example, and the concept that had the most number\n",
        "of examples had 36 phrases. On average, each concept had 1.42 examples. The AskAPatient data set had 4,496 unique phrase-label pairs, 1,036 unique labels while 26 phrases had multiple labels. Table III shows examples of phrases that had multiple labels. For example, ‘mad’ can be mapped to ‘anger’ or ‘rage’ and ‘sore’ can be mapped to ‘pain’ or ‘myalgia’.\n",
        "\n",
        "(2) Second, we remove all concepts that had less than five examples. The statistics of the final data are shown in Table IV.\n",
        "\n",
        "(3) Third, we divide all examples without multiple labels into random 10 folds such that each unique phrase-label pair\n",
        "appears once in one of the 10 test sets. We add the pairs with multiple labels into the training data. This final 10-folds\n",
        "dataset is used in all our experiments.\n",
        "\n",
        "(The original paper can be download on canvas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qMSs5nZxcHg",
        "outputId": "aa9cea0c-f5a0-4a8c-dbc7-3710aae39228"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "#we have upload our dataset to google drive\n",
        "#mounting with our google drive where our datasets are there\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#If we have already ran this script and if we run next it again we will be getting drive already mounted \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(1)\n",
        "# You code here (Please add comments in the code):\n",
        "\n",
        "#Import the libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import glob\n",
        "output_data =  pd.DataFrame()\n",
        "\n",
        "# There are two datasets so we have to iterate through both of them \n",
        "for dataset in ['TwADR-L', 'AskAPatient']:\n",
        "    output = {}\n",
        "    p = pd.DataFrame()\n",
        "    \n",
        "    for filepath in glob.glob('/content/gdrive/My Drive/datasets/{}/*.txt'.format(dataset)): \n",
        "      \n",
        "      #datasets are present in our google drive\n",
        "      \n",
        "        input = pd.read_csv(filepath, sep = \"\\t\", header = None, encoding= 'unicode_escape') #read the datasets\n",
        "        p = p.append(input)\n",
        "\n",
        "    #combine all the text files present in the dataset and form a combined dataset\n",
        "    p = p.reset_index(drop=True)\n",
        "\n",
        "    p['phrase_label'] = p[1] + \" \" + p[2]\n",
        "\n",
        "    p.columns = ['identification', 'names', 'h', 'h_name']\n",
        "#convert to strings\n",
        "   \n",
        "    p = p.astype({\"identification\": str})\n",
        "# there are some uppercase letters lets convert them all to lower case\n",
        "  \n",
        "    for columns in p.columns:\n",
        "        p[columns] = p[columns].str.lower() \n",
        "\n",
        "    # clean the data by dropping the duplicates\n",
        "    p = p.drop_duplicates('h_name')\n",
        "\n",
        "    #Let's store the table values in dictionaries\n",
        "    output['**__Unique_phrases__**'] = len(p['h'].unique())\n",
        "\n",
        "    output['**__Unique_phrase_label_pairs**__'] = p.shape[0]\n",
        "\n",
        "    output['**__Unique_labels__**'] = len(p['names'].unique())\n",
        "\n",
        "    p1 = pd.DataFrame(p['h'].value_counts())\n",
        "\n",
        "    output['**__Phrases with multiple labels__**'] = p1[p1['h'] > 1].shape[0]\n",
        "\n",
        "    output['**__Maximum examples per label__**'] = p['names'].value_counts().values.max()\n",
        "\n",
        "    output['**__Minimum examples per label__**'] = p['names'].value_counts().values.min()\n",
        "\n",
        "    output['**__Average examples per label__**'] = round(p['names'].value_counts().mean(), 2)\n",
        "\n",
        "    output_data = output_data.append(output, ignore_index=True)\n",
        "\n",
        "\n",
        "output_data = output_data.T\n",
        "\n",
        "output_data = output_data.astype({0: int, 1: int})\n",
        "\n",
        "output_data.columns = ['TwADR-L', 'AskAPatient']\n",
        "\n",
        "output_data\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "Z4tYp6iMxqrk",
        "outputId": "2538b738-b4f4-4060-b627-7da50f4a9652"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      TwADR-L  AskAPatient\n",
              "**__Unique_phrases__**                   2944         4470\n",
              "**__Unique_phrase_label_pairs**__        3157         4507\n",
              "**__Unique_labels__**                    2220         1038\n",
              "**__Phrases with multiple labels__**      173           35\n",
              "**__Maximum examples per label__**         36          141\n",
              "**__Minimum examples per label__**          1            1\n",
              "**__Average examples per label__**          1            4"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5e9db7c1-055c-4bd8-95c4-8c8175a06a5b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TwADR-L</th>\n",
              "      <th>AskAPatient</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>**__Unique_phrases__**</th>\n",
              "      <td>2944</td>\n",
              "      <td>4470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>**__Unique_phrase_label_pairs**__</th>\n",
              "      <td>3157</td>\n",
              "      <td>4507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>**__Unique_labels__**</th>\n",
              "      <td>2220</td>\n",
              "      <td>1038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>**__Phrases with multiple labels__**</th>\n",
              "      <td>173</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>**__Maximum examples per label__**</th>\n",
              "      <td>36</td>\n",
              "      <td>141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>**__Minimum examples per label__**</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>**__Average examples per label__**</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5e9db7c1-055c-4bd8-95c4-8c8175a06a5b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5e9db7c1-055c-4bd8-95c4-8c8175a06a5b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5e9db7c1-055c-4bd8-95c4-8c8175a06a5b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (2) Second, we remove all concepts that had less than five examples. The statistics of the final data are shown in Table IV.\n",
        "\n",
        "import glob\n",
        "output_data = pd.DataFrame()\n",
        "\n",
        "# we iterate through both datasets \n",
        "for s in ['TwADR-L', 'AskAPatient']:\n",
        "    p = pd.DataFrame()\n",
        "    output = {}\n",
        "    for filepath in glob.glob('/content/gdrive/My Drive/datasets/{}/*.txt'.format(s)):\n",
        "\n",
        "        #print(filepath)\n",
        "        input = pd.read_csv(filepath, sep = \"\\t\", header = None, encoding= 'unicode_escape')\n",
        "\n",
        "        p = p.append(input)\n",
        "\n",
        "    # we create combined dataframe by combining Each dataset text files.\n",
        "        p = p.reset_index(drop=True)\n",
        "\n",
        "    p['phrase_label'] = p[1] + \" \" + p[2]\n",
        "\n",
        "    #p.columns = ['id', 'labels', 'phrases', 'phrase-label']\n",
        "    p.columns = ['identification', 'names', 'h', 'h_name']\n",
        "    \n",
        "    p = p.astype({\"identification\": str})\n",
        "# there are some uppercase letters lets convert them all to lower case\n",
        "  \n",
        "    for columns in p.columns:\n",
        "        p[columns] = p[columns].str.lower() \n",
        "\n",
        "    # we drop duplicates that are phrase-label pairs... \n",
        "    p = p.drop_duplicates('h_name')\n",
        "\n",
        "    z = []\n",
        "\n",
        "    for i in range(p.shape[0]):\n",
        "        if p['names'].value_counts()[p.iloc[i]['names']] < 5:\n",
        "            z.append(i)\n",
        "\n",
        "    # we drop the labels that have less than count of 5.\n",
        "    p.drop(p.index[z], inplace=True)\n",
        "    output = {}\n",
        "    \n",
        "    # storing results of the table in dictionary\n",
        "    output['__**Unique_phrases**__'] = len(p['h'].unique())\n",
        "\n",
        "    output['__**Unique_labels**__'] = len(p['names'].unique())\n",
        "\n",
        "    output['__**Unique_phrase_label_pairs**__'] = p.shape[0]\n",
        "\n",
        "    p1 = pd.DataFrame(p['h'].value_counts())\n",
        "    output['__**Phrases with multiple labels**__'] = p1[p1['h'] > 1].shape[0]\n",
        "\n",
        "    output['__**Maximum examples per label**__'] = p['names'].value_counts().values.max()\n",
        "\n",
        "    output['__**Minimum examples per label**__'] = p['names'].value_counts().values.min()\n",
        "\n",
        "    output['__**Average examples per label**__'] = round(p['names'].value_counts().mean(), 2)\n",
        "    # appending dictinary to dataframe\n",
        "    \n",
        "    output_data = output_data.append(output, ignore_index=True)\n",
        "\n",
        "output_data = output_data.T\n",
        "\n",
        "output_data = output_data.astype({0: int, 1: int})\n",
        "\n",
        "output_data.columns = ['TwADR-L', 'AskAPatient']\n",
        "\n",
        "output_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "d4YiBGSHxtJi",
        "outputId": "07e9ad5d-ea19-43b4-fd98-d150bc545fc8"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      TwADR-L  AskAPatient\n",
              "__**Unique_phrases**__                    616         2665\n",
              "__**Unique_labels**__                      76          233\n",
              "__**Unique_phrase_label_pairs**__         721         2686\n",
              "__**Phrases with multiple labels**__       87           19\n",
              "__**Maximum examples per label**__         36          141\n",
              "__**Minimum examples per label**__          5            5\n",
              "__**Average examples per label**__          9           11"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dc332f5b-e44e-4894-98e5-38fbfb3d5b72\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TwADR-L</th>\n",
              "      <th>AskAPatient</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>__**Unique_phrases**__</th>\n",
              "      <td>616</td>\n",
              "      <td>2665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>__**Unique_labels**__</th>\n",
              "      <td>76</td>\n",
              "      <td>233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>__**Unique_phrase_label_pairs**__</th>\n",
              "      <td>721</td>\n",
              "      <td>2686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>__**Phrases with multiple labels**__</th>\n",
              "      <td>87</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>__**Maximum examples per label**__</th>\n",
              "      <td>36</td>\n",
              "      <td>141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>__**Minimum examples per label**__</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>__**Average examples per label**__</th>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc332f5b-e44e-4894-98e5-38fbfb3d5b72')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dc332f5b-e44e-4894-98e5-38fbfb3d5b72 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dc332f5b-e44e-4894-98e5-38fbfb3d5b72');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from numpy import unique\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "r = np.array(p)\n",
        "\n",
        "print(\"Result For Question 3\")\n",
        "\n",
        "kf = KFold(n_splits = 10, shuffle = True, random_state = 4312)\n",
        "\n",
        "for u, v in kf.split(r):\n",
        "    print(\"TRAIN_DATA:\", u, \"TEST_DATA:\", v)\n",
        "    train_data, test_data = r[u], r[v]\n",
        "\n",
        "g, f = make_classification(n_samples=1000, n_classes=2, weights=[0.99, 0.01], flip_y=0, random_state=1)\n",
        "\n",
        "\n",
        "rev = unique(f)\n",
        "co = len(f)\n",
        "\n",
        "for i in rev:\n",
        "\tj = len(f[f==i])\n",
        "\top = j / co * 100\n",
        "\n",
        "\n",
        "\tprint('> Class is = %d : %d/%d (%.1f%%)' % (i, op, co, op))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioWtPIqe99Ri",
        "outputId": "d6defad0-f2c8-4503-e6e6-497bbd69011d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result For Question 3\n",
            "TRAIN_DATA: [   0    1    2 ... 2683 2684 2685] TEST_DATA: [  28   29   40   46   63   69   75   85  106  131  142  148  156  158\n",
            "  169  172  177  182  190  208  226  228  243  253  265  267  275  286\n",
            "  311  317  326  331  334  336  339  341  351  356  362  363  380  386\n",
            "  405  408  411  415  427  473  484  503  510  512  526  539  543  553\n",
            "  554  557  565  570  586  595  603  609  616  631  643  666  714  717\n",
            "  734  757  762  797  804  827  842  858  860  874  883  890  909  929\n",
            "  932  935  942  953  971  976  986  996 1018 1019 1048 1065 1070 1080\n",
            " 1088 1091 1092 1104 1105 1114 1119 1139 1146 1150 1153 1174 1176 1185\n",
            " 1188 1220 1223 1274 1293 1306 1310 1311 1314 1317 1321 1322 1324 1359\n",
            " 1366 1373 1381 1388 1392 1399 1406 1417 1442 1463 1473 1488 1493 1495\n",
            " 1496 1497 1499 1503 1505 1510 1521 1536 1548 1570 1573 1579 1580 1582\n",
            " 1603 1606 1608 1626 1628 1645 1649 1654 1655 1662 1667 1670 1701 1702\n",
            " 1709 1782 1785 1790 1792 1818 1843 1845 1846 1856 1868 1878 1879 1890\n",
            " 1892 1895 1896 1919 1923 1926 1933 1939 1940 1944 1946 1968 1976 1977\n",
            " 1979 1984 1988 2012 2015 2038 2041 2049 2075 2085 2093 2128 2145 2158\n",
            " 2160 2173 2176 2194 2209 2219 2223 2225 2226 2229 2257 2282 2283 2286\n",
            " 2296 2304 2336 2342 2344 2347 2353 2386 2390 2399 2403 2417 2425 2428\n",
            " 2430 2453 2455 2458 2464 2484 2487 2492 2493 2494 2495 2504 2507 2510\n",
            " 2524 2525 2526 2529 2542 2545 2558 2580 2591 2600 2614 2615 2630 2645\n",
            " 2647 2648 2663]\n",
            "TRAIN_DATA: [   0    1    3 ... 2682 2684 2685] TEST_DATA: [   2   13   14   24   34   36   43   62   66   78   84   95   97  103\n",
            "  117  118  126  151  153  166  171  173  176  186  187  195  196  198\n",
            "  202  211  244  259  271  274  281  291  295  297  307  310  315  327\n",
            "  332  400  401  425  463  478  506  509  514  519  524  533  555  561\n",
            "  568  571  577  578  580  587  588  612  613  622  630  632  633  634\n",
            "  635  649  661  674  690  695  699  730  735  738  740  745  747  751\n",
            "  768  776  788  801  815  817  835  838  841  856  859  865  869  882\n",
            "  889  896  905  916  928  930  939  957  962  966  974  980  987  989\n",
            " 1016 1021 1058 1062 1064 1103 1108 1128 1132 1138 1142 1151 1154 1157\n",
            " 1160 1184 1189 1196 1228 1245 1249 1278 1281 1297 1304 1323 1338 1351\n",
            " 1375 1416 1421 1430 1436 1437 1453 1455 1462 1470 1484 1507 1515 1518\n",
            " 1522 1546 1549 1555 1575 1587 1598 1607 1610 1615 1627 1636 1657 1660\n",
            " 1675 1695 1703 1705 1708 1713 1715 1721 1742 1747 1752 1756 1758 1760\n",
            " 1777 1779 1784 1794 1798 1799 1805 1806 1825 1840 1844 1848 1853 1867\n",
            " 1869 1894 1899 1934 1950 1953 1956 1962 1971 1983 1992 2007 2010 2014\n",
            " 2021 2022 2030 2036 2042 2047 2050 2052 2053 2065 2070 2086 2088 2092\n",
            " 2098 2101 2108 2109 2110 2118 2131 2138 2167 2171 2210 2211 2216 2224\n",
            " 2246 2254 2264 2265 2278 2279 2317 2330 2340 2356 2376 2388 2394 2412\n",
            " 2433 2437 2477 2479 2509 2536 2544 2549 2555 2563 2597 2611 2621 2639\n",
            " 2671 2674 2683]\n",
            "TRAIN_DATA: [   0    1    2 ... 2683 2684 2685] TEST_DATA: [   5   16   17   22   26   44   49   73   86   91   96  114  119  124\n",
            "  136  138  147  157  184  191  200  201  205  217  219  224  229  238\n",
            "  252  279  280  287  289  304  305  320  323  348  352  355  358  388\n",
            "  397  399  414  417  431  452  458  459  460  464  466  475  476  481\n",
            "  500  507  511  530  532  538  547  556  563  590  592  611  646  648\n",
            "  656  665  676  677  685  687  694  703  750  752  755  770  779  790\n",
            "  798  814  825  832  847  848  849  850  864  868  876  893  927  943\n",
            "  949  952  956  958  968  983 1007 1009 1035 1044 1047 1052 1067 1068\n",
            " 1071 1082 1086 1089 1096 1097 1127 1129 1136 1143 1161 1163 1170 1177\n",
            " 1180 1183 1204 1207 1217 1225 1242 1246 1250 1280 1316 1320 1331 1334\n",
            " 1345 1348 1356 1357 1358 1365 1369 1371 1378 1380 1397 1398 1400 1414\n",
            " 1415 1420 1422 1474 1478 1501 1520 1530 1535 1562 1565 1588 1602 1617\n",
            " 1629 1637 1659 1672 1673 1696 1700 1716 1725 1761 1764 1772 1774 1778\n",
            " 1786 1807 1812 1814 1826 1838 1861 1866 1870 1891 1991 1994 1997 2004\n",
            " 2016 2027 2032 2034 2040 2043 2051 2056 2078 2083 2087 2099 2106 2133\n",
            " 2136 2152 2153 2162 2179 2185 2186 2195 2202 2206 2217 2221 2238 2240\n",
            " 2274 2289 2293 2301 2315 2316 2324 2327 2362 2363 2367 2370 2372 2382\n",
            " 2393 2406 2409 2413 2419 2429 2452 2457 2465 2471 2481 2505 2511 2512\n",
            " 2531 2535 2565 2573 2577 2592 2598 2599 2602 2605 2613 2633 2637 2643\n",
            " 2657 2659 2681]\n",
            "TRAIN_DATA: [   0    2    4 ... 2682 2683 2685] TEST_DATA: [   1    3   21   25   48   70   72   93  111  125  127  128  133  134\n",
            "  141  145  160  170  178  181  199  204  213  220  230  239  247  250\n",
            "  256  273  312  314  333  359  361  377  381  382  395  407  413  433\n",
            "  435  436  438  441  445  447  456  462  467  485  498  501  502  513\n",
            "  550  567  569  576  591  604  618  620  625  639  644  645  651  658\n",
            "  672  680  689  708  716  726  732  739  742  756  771  773  784  785\n",
            "  786  796  816  822  839  852  863  871  892  899  915  924  926  937\n",
            "  947  954  959  967  982  993  994 1006 1010 1017 1036 1045 1046 1055\n",
            " 1056 1061 1078 1100 1115 1122 1172 1209 1215 1227 1236 1239 1248 1256\n",
            " 1257 1267 1289 1294 1303 1339 1350 1367 1386 1391 1395 1401 1404 1409\n",
            " 1432 1444 1471 1477 1483 1492 1504 1506 1512 1534 1544 1552 1557 1560\n",
            " 1561 1564 1567 1585 1592 1601 1646 1674 1676 1680 1688 1690 1691 1706\n",
            " 1714 1717 1740 1744 1749 1762 1767 1768 1770 1783 1809 1822 1831 1837\n",
            " 1847 1859 1874 1882 1893 1903 1908 1915 1917 1936 1949 1954 1966 2002\n",
            " 2006 2011 2026 2028 2033 2037 2055 2061 2071 2084 2094 2096 2100 2107\n",
            " 2122 2124 2135 2164 2170 2192 2197 2198 2200 2215 2220 2227 2248 2249\n",
            " 2259 2262 2268 2287 2297 2303 2313 2319 2320 2326 2352 2354 2377 2389\n",
            " 2398 2410 2415 2435 2460 2483 2508 2516 2517 2520 2521 2530 2546 2552\n",
            " 2562 2570 2571 2581 2583 2589 2616 2624 2627 2636 2638 2649 2651 2653\n",
            " 2666 2668 2684]\n",
            "TRAIN_DATA: [   0    1    2 ... 2683 2684 2685] TEST_DATA: [  11   15   30   32   42   47   65   71   87   89  112  121  143  150\n",
            "  159  163  215  260  262  270  284  294  298  300  313  338  345  367\n",
            "  379  392  394  410  420  421  423  424  429  443  454  480  486  491\n",
            "  494  496  499  504  518  521  545  552  559  564  573  601  610  624\n",
            "  626  641  652  657  660  673  678  686  691  704  706  723  727  736\n",
            "  737  744  746  753  760  769  775  778  787  793  800  812  819  826\n",
            "  831  837  840  846  854  867  872  877  878  888  897  903  907  908\n",
            "  919  925  941  964  988  990  998 1002 1004 1013 1025 1027 1033 1039\n",
            " 1041 1042 1057 1059 1066 1095 1106 1112 1117 1118 1125 1148 1149 1156\n",
            " 1168 1171 1175 1191 1198 1218 1221 1232 1241 1261 1269 1282 1284 1307\n",
            " 1319 1337 1347 1355 1364 1370 1374 1379 1389 1396 1429 1433 1435 1454\n",
            " 1459 1487 1553 1571 1605 1622 1632 1638 1658 1681 1682 1692 1693 1697\n",
            " 1723 1729 1732 1734 1739 1755 1766 1788 1804 1810 1811 1849 1850 1854\n",
            " 1860 1864 1872 1875 1877 1885 1887 1902 1935 1941 1942 1965 1970 1982\n",
            " 1985 1986 1998 2008 2017 2020 2025 2031 2046 2048 2057 2104 2116 2129\n",
            " 2139 2147 2161 2169 2172 2180 2181 2193 2214 2218 2231 2244 2251 2281\n",
            " 2290 2292 2309 2318 2341 2357 2359 2365 2374 2375 2383 2387 2392 2396\n",
            " 2397 2401 2408 2411 2440 2446 2473 2474 2491 2498 2500 2501 2502 2519\n",
            " 2537 2550 2554 2568 2572 2576 2578 2588 2601 2608 2612 2619 2632 2650\n",
            " 2656 2667 2676]\n",
            "TRAIN_DATA: [   0    1    2 ... 2683 2684 2685] TEST_DATA: [  19   37   38   41   55   76   77   88   90   99  104  129  130  132\n",
            "  139  140  144  155  161  165  183  185  193  194  206  223  225  231\n",
            "  242  251  254  264  277  288  293  296  318  321  322  335  346  349\n",
            "  364  365  372  391  402  409  418  422  428  437  449  489  516  523\n",
            "  525  527  560  579  596  606  617  621  642  647  650  653  662  664\n",
            "  667  668  670  679  682  683  693  731  749  758  759  764  767  780\n",
            "  781  807  809  829  833  844  845  887  901  918  922  934  940  944\n",
            "  945  973  977  978  979  985 1031 1037 1040 1051 1076 1083 1093 1094\n",
            " 1109 1113 1126 1130 1134 1144 1158 1187 1193 1194 1197 1201 1202 1206\n",
            " 1229 1244 1263 1268 1275 1276 1287 1288 1301 1318 1342 1343 1382 1410\n",
            " 1425 1431 1438 1439 1440 1441 1448 1451 1456 1481 1482 1486 1494 1508\n",
            " 1509 1537 1538 1554 1559 1577 1589 1594 1609 1616 1618 1624 1631 1643\n",
            " 1647 1652 1665 1671 1686 1698 1704 1748 1750 1776 1781 1791 1796 1816\n",
            " 1858 1881 1897 1916 1925 1932 1938 1943 1952 1960 1972 1974 1975 1989\n",
            " 1990 2029 2045 2059 2060 2063 2064 2066 2079 2089 2112 2140 2144 2149\n",
            " 2151 2155 2159 2163 2166 2168 2177 2183 2184 2203 2208 2239 2241 2255\n",
            " 2256 2273 2288 2291 2298 2299 2300 2306 2335 2338 2349 2350 2358 2360\n",
            " 2402 2407 2416 2434 2436 2441 2442 2444 2448 2451 2461 2467 2468 2482\n",
            " 2488 2489 2506 2515 2528 2533 2551 2557 2569 2575 2579 2594 2625 2641\n",
            " 2661 2673 2679]\n",
            "TRAIN_DATA: [   1    2    3 ... 2683 2684 2685] TEST_DATA: [   0    4    6   18   23   45   52   56   57   60   68   82   83  120\n",
            "  146  149  152  154  168  174  203  207  210  212  218  222  234  235\n",
            "  237  245  255  268  269  282  301  309  329  337  344  350  353  354\n",
            "  366  374  403  419  430  434  444  448  457  471  474  477  482  483\n",
            "  488  493  508  517  520  528  531  537  544  558  574  583  597  605\n",
            "  623  627  628  638  655  659  671  688  697  701  705  713  720  724\n",
            "  725  754  761  782  802  806  808  811  824  834  836  853  855  879\n",
            "  880  881  886  894  902  914  931  946  948  960  981  984 1000 1011\n",
            " 1028 1049 1050 1053 1073 1074 1075 1077 1087 1090 1098 1107 1116 1120\n",
            " 1131 1133 1145 1159 1181 1186 1205 1213 1222 1226 1231 1252 1255 1260\n",
            " 1265 1286 1292 1296 1305 1333 1361 1387 1408 1427 1452 1458 1480 1485\n",
            " 1490 1500 1514 1526 1527 1539 1551 1574 1576 1595 1641 1664 1720 1724\n",
            " 1728 1730 1745 1754 1763 1797 1813 1819 1821 1824 1830 1832 1833 1835\n",
            " 1839 1851 1865 1871 1880 1888 1900 1901 1912 1918 1922 1927 1947 1948\n",
            " 1958 1963 1967 1969 1993 1996 2000 2009 2013 2039 2073 2115 2130 2134\n",
            " 2142 2146 2148 2157 2165 2188 2191 2201 2230 2243 2245 2252 2260 2261\n",
            " 2263 2266 2270 2271 2302 2323 2333 2343 2379 2381 2385 2391 2414 2427\n",
            " 2431 2432 2438 2454 2466 2470 2485 2486 2496 2499 2522 2523 2539 2540\n",
            " 2548 2567 2582 2585 2587 2593 2603 2610 2622 2623 2626 2631 2642 2644\n",
            " 2662 2670]\n",
            "TRAIN_DATA: [   0    1    2 ... 2683 2684 2685] TEST_DATA: [   9   27   33   35   39   50   51   58   59   64   79   94   98  100\n",
            "  109  116  123  135  164  175  179  188  197  209  214  236  240  241\n",
            "  257  258  263  266  290  302  306  316  325  340  342  343  357  360\n",
            "  368  371  373  375  376  393  404  442  465  470  472  495  536  546\n",
            "  551  566  589  594  599  600  615  636  684  719  721  722  743  748\n",
            "  763  765  777  783  789  799  820  821  828  843  851  885  895  906\n",
            "  910  912  923  936  950  963  972  999 1001 1008 1014 1024 1026 1032\n",
            " 1034 1038 1054 1069 1084 1099 1101 1110 1121 1123 1135 1137 1140 1152\n",
            " 1162 1165 1166 1173 1179 1200 1219 1224 1230 1234 1240 1243 1247 1251\n",
            " 1253 1262 1272 1273 1279 1312 1325 1328 1329 1330 1353 1377 1384 1403\n",
            " 1418 1424 1426 1449 1460 1461 1466 1468 1472 1476 1479 1513 1517 1519\n",
            " 1524 1531 1532 1540 1542 1581 1590 1593 1599 1600 1604 1614 1623 1639\n",
            " 1666 1677 1685 1687 1694 1699 1710 1722 1731 1735 1736 1746 1753 1775\n",
            " 1787 1793 1800 1801 1834 1852 1857 1883 1889 1898 1904 1914 1921 1924\n",
            " 1931 1959 1964 1980 1981 1987 2035 2062 2072 2074 2082 2090 2091 2113\n",
            " 2127 2150 2156 2182 2189 2196 2199 2204 2205 2222 2232 2233 2237 2247\n",
            " 2258 2267 2275 2276 2277 2285 2294 2295 2322 2328 2329 2331 2337 2346\n",
            " 2364 2373 2378 2380 2384 2418 2426 2439 2450 2490 2497 2503 2534 2541\n",
            " 2543 2559 2560 2564 2566 2574 2584 2586 2607 2617 2620 2646 2660 2664\n",
            " 2678 2682]\n",
            "TRAIN_DATA: [   0    1    2 ... 2682 2683 2684] TEST_DATA: [   7    8   20   31   53   54   67   80   81  101  105  110  113  162\n",
            "  167  180  192  233  246  248  272  278  283  285  292  299  328  330\n",
            "  370  378  383  387  389  396  412  426  440  446  450  451  469  505\n",
            "  522  541  548  562  572  584  602  607  608  640  675  681  696  698\n",
            "  700  702  709  712  715  728  729  733  741  766  774  791  794  795\n",
            "  805  810  823  866  873  875  891  904  911  920  965  970  975  991\n",
            "  992  997 1012 1015 1029 1030 1043 1072 1079 1085 1155 1164 1178 1182\n",
            " 1190 1195 1199 1203 1211 1212 1216 1233 1237 1254 1266 1277 1285 1298\n",
            " 1302 1332 1335 1340 1341 1344 1346 1349 1352 1354 1360 1362 1368 1372\n",
            " 1383 1393 1394 1405 1407 1411 1434 1443 1445 1450 1457 1464 1469 1475\n",
            " 1502 1511 1523 1525 1529 1533 1543 1547 1550 1556 1566 1568 1572 1591\n",
            " 1611 1612 1620 1625 1634 1642 1644 1653 1656 1661 1668 1669 1678 1684\n",
            " 1689 1707 1712 1718 1719 1726 1727 1738 1743 1757 1759 1765 1771 1789\n",
            " 1808 1815 1820 1823 1829 1841 1842 1863 1876 1886 1909 1910 1913 1920\n",
            " 1928 1929 1930 1937 1951 1955 1957 1961 1973 2005 2023 2044 2054 2058\n",
            " 2067 2077 2111 2120 2121 2125 2132 2141 2143 2154 2174 2178 2187 2207\n",
            " 2235 2242 2253 2269 2272 2280 2284 2308 2310 2311 2321 2332 2334 2339\n",
            " 2345 2348 2351 2355 2368 2404 2405 2447 2449 2456 2469 2472 2475 2476\n",
            " 2480 2532 2538 2553 2556 2561 2590 2595 2604 2606 2609 2629 2634 2635\n",
            " 2680 2685]\n",
            "TRAIN_DATA: [   0    1    2 ... 2683 2684 2685] TEST_DATA: [  10   12   61   74   92  102  107  108  115  122  137  189  216  221\n",
            "  227  232  249  261  276  303  308  319  324  347  369  384  385  390\n",
            "  398  406  416  432  439  453  455  461  468  479  487  490  492  497\n",
            "  515  529  534  535  540  542  549  575  581  582  585  593  598  614\n",
            "  619  629  637  654  663  669  692  707  710  711  718  772  792  803\n",
            "  813  818  830  857  861  862  870  884  898  900  913  917  921  933\n",
            "  938  951  955  961  969  995 1003 1005 1020 1022 1023 1060 1063 1081\n",
            " 1102 1111 1124 1141 1147 1167 1169 1192 1208 1210 1214 1235 1238 1258\n",
            " 1259 1264 1270 1271 1283 1290 1291 1295 1299 1300 1308 1309 1313 1315\n",
            " 1326 1327 1336 1363 1376 1385 1390 1402 1412 1413 1419 1423 1428 1446\n",
            " 1447 1465 1467 1489 1491 1498 1516 1528 1541 1545 1558 1563 1569 1578\n",
            " 1583 1584 1586 1596 1597 1613 1619 1621 1630 1633 1635 1640 1648 1650\n",
            " 1651 1663 1679 1683 1711 1733 1737 1741 1751 1769 1773 1780 1795 1802\n",
            " 1803 1817 1827 1828 1836 1855 1862 1873 1884 1905 1906 1907 1911 1945\n",
            " 1978 1995 1999 2001 2003 2018 2019 2024 2068 2069 2076 2080 2081 2095\n",
            " 2097 2102 2103 2105 2114 2117 2119 2123 2126 2137 2175 2190 2212 2213\n",
            " 2228 2234 2236 2250 2305 2307 2312 2314 2325 2361 2366 2369 2371 2395\n",
            " 2400 2420 2421 2422 2423 2424 2443 2445 2459 2462 2463 2478 2513 2514\n",
            " 2518 2527 2547 2596 2618 2628 2640 2652 2654 2655 2658 2665 2669 2672\n",
            " 2675 2677]\n",
            "> Class is = 0 : 99/1000 (99.0%)\n",
            "> Class is = 1 : 1/1000 (1.0%)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "lab_assignment_03.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}